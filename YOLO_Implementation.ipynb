{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports OpenCV (Open Source Computer Vision Library) and Numerical Python\n",
    "\n",
    "Helpful resource: https://learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializes the YOLO object detection model version 3 by loading the pre-trained weights and configuration from https://github.com/pjreddie/darknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net is the YOLO Nueral Network trained by the YOLOv3 weights and config from GitHub.\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets the names of the output layers needed for object detection. Checks compatibility with OpenCV versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the names of all the layers in the YOLO Neural Network\n",
    "layer_names = net.getLayerNames()\n",
    "\n",
    "'''\n",
    "    try: expects to return a list of lists \n",
    "        where each sub-list contains a single index.\n",
    "    \n",
    "    except: expects to returns a flat array of indices\n",
    "        not wrapped in sub-lists.\n",
    "        \n",
    "    [layer_names[i[0] - 1] is used because OpenCV is index-1 based\n",
    "        python is index-0 based.\n",
    "'''\n",
    "try:\n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "except:\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads the class labels that YOLO was trained on (COCO.names also from https://github.com/pjreddie/darknet). Handles FileNotFound error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "\n",
    "try:\n",
    "    with open(\"coco.names\", \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'coco.names' file not found.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepares the video file for object detection. Handles failure to open file and failure to properly read frames of video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'CityScene.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "'''\n",
    "        ret is true if the frames of the video are read correctly.\n",
    "        cap.read() is iterating through the frames.\n",
    "        Each frame is an image represented by an array.\n",
    "    '''\n",
    "ret, frame = cap.read()\n",
    "\n",
    "if not ret:\n",
    "    print(\"Failed to grab a frame from the video.\")\n",
    "    cap.release()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loop for Video Processing: The infinite loop is used to process each frame of the video sequentially until the video ends or the loop is manually terminated by the user (q).\n",
    "\n",
    "2. Frame Acquisition: Each iteration of the loop reads the next frame from the video. If there are no more frames to read (ret is False), the loop breaks, ending the processing.\n",
    "\n",
    "3. Frame to Blob: The current frame is transformed into a blob (preprocessed image format compatible with the neural network). This scales the image and subtracts mean values.\n",
    "\n",
    "4. Forward Pass: The blob is fed into the YOLO network by setting it as the input and then performing a forward pass to obtain the detection results in outs.\n",
    "\n",
    "5. Processing Detections: YOLO:\n",
    "        Confidence scores and class IDs.\n",
    "        Filter out detections with confidence below a threshold.\n",
    "        Calculate the bounding box coordinates.\n",
    "        Store the coordinates, confidence scores, and class IDs.\n",
    "\n",
    "6. Non-Max Suppression: Applies suppression to reduce overlapping bounding boxes.\n",
    "\n",
    "7. Drawing Bounding Boxes and Labels: For each detection after suppression:\n",
    "        Draw a bounding box around the detected object.\n",
    "        Put a label with the class name and confidence score on the bounding box.\n",
    "\n",
    "8. Displaying the Frame: Displays an annotated frame with boxes and labels.\n",
    "\n",
    "9. End: Releases the capture and destroys all OpenCV windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    \n",
    "    '''\n",
    "        ret is true if the frames of the video are read correctly.\n",
    "        cap.read() is iterating through the frames.\n",
    "        Each frame is an image represented by an array.\n",
    "    '''\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break conditions for the while loop\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    '''\n",
    "        1/255 = 0.00392 for normalization\n",
    "        YOLOv3 works with 416x416 pixel images\n",
    "        The True converts BGR to RGB\n",
    "        OpenCV reads images in BGR and YOLO uses RGB\n",
    "    '''\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    \n",
    "    # Sets up 'blob' as the input for the YOLO network 'net'\n",
    "    net.setInput(blob)\n",
    "    \n",
    "    # Results are stored in 'outs' for a forward pass through the model\n",
    "    outs = net.forward(output_layers)\n",
    "    \n",
    "    # Stores classification ids, confidence scores and box coordinates\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            \n",
    "            '''\n",
    "                scores = detection[5:] gets the class confidence scores for the detection. \n",
    "                The first four elements in detection[] are the bounding box coordinates. \n",
    "                The class confidence scores start from the fifth element.\n",
    "            '''\n",
    "            scores = detection[5:]\n",
    "            \n",
    "            # argmax(scores) identifies the highest score (most likely image)\n",
    "            class_id = np.argmax(scores)\n",
    "            \n",
    "            # if scores[class_id] is greater than 0.5, its considered a valid detection; otherwise its ignored\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                \n",
    "                '''\n",
    "                    center_x and center_y are center coordinates of the bounding box.\n",
    "                    w and h are the width and height of the bounding box.\n",
    "                    x and y are the top-left coordinates of the bounding box.\n",
    "                    These are scaled to the frames dimensions.\n",
    "                '''\n",
    "                center_x = int(detection[0] * frame.shape[1])\n",
    "                center_y = int(detection[1] * frame.shape[0])\n",
    "                w = int(detection[2] * frame.shape[1])\n",
    "                h = int(detection[3] * frame.shape[0])\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    '''\n",
    "        NMSBoxes performs Non-Maximum Suppression on bounding boxes.\n",
    "        0.5: The confidence threshold. \n",
    "        Only boxes with a confidence score higher than this threshold are Suppressed (if needed). \n",
    "        This removes considering weak detections.\n",
    "        0.4 is the NMS threshold. It determines which boxes are considered for merging.\n",
    "        This is applied for overlapping boxes.\n",
    "        This helps determine if its actually two objects or just one.\n",
    "    '''\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    \n",
    "    '''\n",
    "        flatten() flattens the list of lists that can result from NSMBoxes\n",
    "        iterates through the best bounding boxes in boxes[]\n",
    "            and through the confidence of the prediction in confidences[]\n",
    "    '''\n",
    "    for i in indices.flatten():\n",
    "        box = boxes[i]\n",
    "        # x, y, w, h are the indices\n",
    "        x, y, w, h = box\n",
    "        label = str(classes[class_ids[i]])\n",
    "        confidence = confidences[i]\n",
    "        \n",
    "        '''\n",
    "            .rectangle draws the bounding boxes\n",
    "            (0, 255, 0) changes the color in RGB format, green in this instance.\n",
    "            2 is the thickness of boundary-box.\n",
    "        '''\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        '''\n",
    "            .putText labels the boxes with the best prediction and confidence percent.\n",
    "            (x, y + 30) the text is positioned 30 pixels above box.\n",
    "            the numbers can be fine-tuned depending on the nature of the video\n",
    "        '''\n",
    "        cv2.putText(frame, f\"{label} {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"YOLO Object Detection\", frame)\n",
    "    \n",
    "    # ends the frame if key 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "# ends the cv2 window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
